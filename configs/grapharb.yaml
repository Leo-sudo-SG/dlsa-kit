# GraphArb Configuration
# Spatiotemporal Graph Transformer for portfolio signal extraction
# 
# To run: python3 run_train_test.py -c configs/grapharb.yaml

# Major parameters
mode: "estimate"  # 'test' or 'estimate'
results_tag: "grapharb-demo"
debug: False

# Model parameters
model_name: "GraphArb"  # Must match class name in models/
model: {
  d_model: 64,              # latent embedding dimension
  lstm_hidden: 64,          # LSTM hidden state dimension
  gnn_layers: 2,            # number of Graph Transformer layers
  edge_dim: 1,              # width of edge attributes (correlation scalar)
  topk: 16,                 # top-k neighbors in correlation-based graph
  dropout: 0.1,             # dropout rate
  lookback: 30,             # number of days in temporal window
}

# Data parameters
preprocess_func: "preprocess_cumsum"  # preprocessing function from preprocess.py
use_residual_weights: False  # Set to False since residual weight files don't exist for IPCA
cap_proportion: 0.01  # 0.01 = top 1% cap (filters to ~2.4k assets from ~9.5k via superMask)
factor_models: {
  "IPCA": [1],          # Try 1 factor for quick test; can expand to [1, 3, 5, 8, 10, 15]
}
perturbation: {}  # Leave empty to disable noise injection

# Training parameters
num_epochs: 1  # Small for testing; increase for production
optimizer_name: "Adam"
optimizer_opts: {
  lr: 0.001
}
batch_size: 128
retrain_freq: 250  # test set size in estimate mode
rolling_retrain: True
force_retrain: True
length_training: 250  # in-sample training window
early_stopping: False
objective: "sharpe"  # objective: 'sharpe', 'meanvar', or 'sqrtMeanSharpe'

# Market frictions
market_frictions: False
trans_cost: 0  # in bps (0.0005 = 5 bps)
hold_cost: 0   # in bps for shorts
